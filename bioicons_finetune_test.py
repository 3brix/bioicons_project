import itertools
from dataclasses import dataclass
from pathlib import Path

import modal
from fastapi import FastAPI
from fastapi.responses import FileResponse
from pathlib import Path

# Name your Modal app
app = modal.App(name="bioicons-flux")

# Create a Modal image with Python + necessary ML libraries
image = modal.Image.debian_slim(python_version="3.10").pip_install(
    "accelerate==0.31.0",
    "datasets==3.1.0",
    "ftfy~=6.1.0",
    "gradio~=4.44.1",
    "fastapi[standard]==0.115.4",
    "numpy==1.26.4",
    "pydantic==2.9.2",
    "starlette==0.41.2",
    "smart_open~=6.4.0",
    "transformers~=4.41.2",
    "sentencepiece>=0.1.91,!=0.1.92",
    "torch~=2.2.0",
    "torchvision~=0.16",
    "triton~=2.2.0",
    "peft==0.11.1",
    "wandb==0.17.6",
    "Pillow",
    "cairosvg",
)

# add specific version of diffusers library
GIT_SHA = (
    "2541d141d5ffa9c94a7e8f5ca7f4ada26bad811d"  # specify the commit to fetch
)

image = (
    image.apt_install("git")
    # Perform a shallow fetch of just the target `diffusers` commit, checking out
    # the commit in the container's home directory, /root. Then install `diffusers`
    .run_commands(
        "cd /root && git init .",
        "cd /root && git remote add origin https://github.com/huggingface/diffusers",
        f"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}",
        "cd /root && pip install -e .",
    )
)

@dataclass
class SharedConfig:
    """Configuration information shared across project components."""

    # The instance name is the "proper noun" we're teaching the model
    instance_name: str = "bioicon"
    class_name: str = "style"
    # identifier for pretrained models on Hugging Face
    model_name: str = "black-forest-labs/FLUX.1-dev"


# downloading pretrained model weights with `run_function`
def download_models():
    from diffusers import DiffusionPipeline
    from transformers.utils import move_cache

    config = SharedConfig()

    DiffusionPipeline.from_pretrained(config.model_name, force_download=True)
    move_cache()

# runs download_models to fetch FLUX.1-dev weights
image = image.run_function(
    download_models, secrets=[modal.Secret.from_name("huggingface-secret")]
)

# add local assets directory to image for web app
web_image = image



# Storing data generated by our app with `modal.Volume`
volume = modal.Volume.from_name(
    "dreambooth-finetuning-volume-flux-hypersweep-bioicons",
    create_if_missing=True,
)
MODEL_DIR = "/model"


# dataset volume (i already uploaded it using the upload_dataset.py), if i understand it well it stores data nad can be fetched by others, but pls try out
dataset_volume = modal.Volume.from_name("bioicons", create_if_missing=True)
DATASET_PATH = "/mnt/bioicons"

# sanity check
def load_bioicons_dataset(dataset_path: str):
    images, captions = [], []
    dataset_dir = Path(dataset_path)
    for img_file in dataset_dir.glob("*.png"):
        txt_file = img_file.with_suffix(".txt")
        if txt_file.exists():
            images.append(img_file)
            with open(txt_file, "r") as f:
                captions.append(f.read().strip())
    print(f"Loaded {len(images)} images from {dataset_dir}")
    return images, captions


# This example can optionally use [Weights & Biases](https://wandb.ai) to track all of this training information.
# Just sign up for an account, switch the flag below, and add your API key as a [Modal secret](https://modal.com/docs/guide/secrets).

USE_WANDB = True



@dataclass
class TrainConfig(SharedConfig):
    """Configuration for the finetuning step."""

    dataset_name = DATASET_PATH
    caption_column = "text"

    instance_prompt = "flat vector icon, white background"  
    # training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`
    prefix: str = ""
    postfix: str = ""

    # Hyperparameters/constants (based on the advice in the book and histopathology exercise)
    resolution: int = 1024
    train_batch_size: int = 1
    rank: int = 16  # LoRA rank
    gradient_accumulation_steps: int = 2
    learning_rate: float = 1e-4
    lr_scheduler: str = "constant"
    lr_warmup_steps: int = 50
    max_train_steps: int = 1000   
    checkpointing_steps: int = 500
    seed: int = 0

# sweep setup
@dataclass
class SweepConfig(TrainConfig):
    """Configuration for hyperparameter sweep"""
    
    learning_rates = [1e-4]
    # learning_rates = [1e-4, 2e-4, 3e-4]
    train_steps = [1000]
    # train_steps = [1000, 1500, 3000, 4000]
    ranks = [16]
    # ranks = [4, 8, 16] ?

    bioicon_test_prompts = [
        "a bioicon style illustration of a jellyfish, flat vector icon, white background",
        "a bioicon style illustration of a dinosaur skull, flat vector icon, white background",
        "a bioicon style illustration of a neuron, flat vector icon, white background",
        "a bioicon style illustration of a virus particle, flat vector icon, white background",
        "a bioicon style illustration of a leaf cross section, flat vector icon, white background",
        "a bioicon style illustration of a snail, flat vector icon, white background",
        "a bioicon style illustration of a bat, flat vector icon, white background",
        "a bioicon style illustration of a deep sea fish, flat vector icon, white background",
        "a bioicon style illustration of a frog, flat vector icon, white background",
        "a bioicon style illustration of a crab, flat vector icon, white background",
        "a bioicon style illustration of a beetle, flat vector icon, white background",
        "a bioicon style illustration of a bird, flat vector icon, white background",
        "a bioicon style illustration of a lion, flat vector icon, white background",
        "a bioicon style illustration of a turtle, flat vector icon, white background",
        "a bioicon style illustration of a butterfly, flat vector icon, white background",
    ]



def generate_sweep_configs(sweep_config: SweepConfig):
    """Generate all combinations of hyperparameters"""
    param_combinations = itertools.product(
        sweep_config.learning_rates,
        sweep_config.train_steps,
        sweep_config.ranks,
    )

    return [
        {
            "learning_rate": lr,
            "max_train_steps": steps,
            "rank": rank,
            "model_name": sweep_config.model_name,
            "instance_prompt": sweep_config.instance_prompt,
            "dataset_name": sweep_config.dataset_name,
            "caption_column": sweep_config.caption_column,
            "resolution": sweep_config.resolution,
            "train_batch_size": sweep_config.train_batch_size,
            "gradient_accumulation_steps": sweep_config.gradient_accumulation_steps,
            "lr_scheduler": sweep_config.lr_scheduler,
            "lr_warmup_steps": sweep_config.lr_warmup_steps,
            "checkpointing_steps": sweep_config.checkpointing_steps,
            "seed": sweep_config.seed,
            "output_dir": Path(MODEL_DIR)
            / f"lr_{lr}_steps_{steps}_rank_{rank}",
        }
        for lr, steps, rank in param_combinations
    ]


@app.function(
    image=image,
    gpu="A100-80GB",
    volumes={MODEL_DIR: volume, DATASET_PATH: dataset_volume},  # stores fine-tuned model
    timeout=7200,  
    secrets=[
        modal.Secret.from_name("wandb"),
        modal.Secret.from_name("huggingface-secret"),
    ]
    if USE_WANDB
    else [modal.Secret.from_name("huggingface-secret")],
)

def train(config):
    import subprocess
    from accelerate.utils import write_basic_config

    # load data locally

    # set up hugging face accelerate library for fast training
    write_basic_config(mixed_precision="bf16")

    # the model training is packaged as a script, so we have to execute it as a subprocess, which adds some boilerplate
    def _exec_subprocess(cmd: list[str]):
        """Executes subprocess and prints log to terminal while subprocess is running."""
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        with process.stdout as pipe:
            for line in iter(pipe.readline, b""):
                line_str = line.decode()
                print(f"{line_str}", end="")

        if exitcode := process.wait() != 0:
            raise subprocess.CalledProcessError(exitcode, "\n".join(cmd))

    # run training -- see huggingface accelerate docs for details
    print("launching dreambooth training script")
    _exec_subprocess(
        [
            "accelerate",
            "launch",
            "examples/dreambooth/train_dreambooth_lora_flux.py",
            "--mixed_precision=bf16",  # half-precision floats most of the time for faster training
            f"--pretrained_model_name_or_path={config['model_name']}",
            #f"--dataset_name={config['dataset_name']}",
            #f"--instance_data_dir={config['dataset_name']}",
            f"--instance_data_dir={DATASET_PATH}",
            f"--caption_column={config['caption_column']}",
            f"--output_dir={config['output_dir']}",
            f"--instance_prompt={config['instance_prompt']}",
            f"--resolution={config['resolution']}",
            f"--train_batch_size={config['train_batch_size']}",
            f"--gradient_accumulation_steps={config['gradient_accumulation_steps']}",
            f"--learning_rate={config['learning_rate']}",
            f"--lr_scheduler={config['lr_scheduler']}",
            f"--lr_warmup_steps={config['lr_warmup_steps']}",
            f"--max_train_steps={config['max_train_steps']}",
            f"--checkpointing_steps={config['checkpointing_steps']}",
            f"--rank={config['rank']}",
            f"--seed={config['seed']}",  # increased reproducibility by seeding the RNG
        ]
        + (
            [
                "--report_to=wandb",
                # validation output tracking is useful, but currently broken for Flux LoRA training
                # f"--validation_prompt={prompt} in space",  # simple test prompt
                # f"--validation_epochs={config['max_train_steps'] // 5}",
            ]
            if USE_WANDB
            else []
        ),
    )
    # The trained model information has been output to the volume mounted at `MODEL_DIR`.
    # To persist this data for use in our web app, we 'commit' the changes
    # to the volume.
    volume.commit()
    return config

# Running our model
@app.cls(image=image, gpu="A100", volumes={MODEL_DIR: volume})
class Model:
    hyperparameter_model_dir: str = modal.parameter()

    @modal.enter()
    def load_model(self):
        import torch
        from diffusers import DiffusionPipeline

        # Reload the modal.Volume to ensure the latest state is accessible.
        volume.reload()

        # set up a hugging face inference pipeline using our model
        pipe = DiffusionPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev",
            torch_dtype=torch.bfloat16,
        ).to("cuda")
        pipe.load_lora_weights(f"{MODEL_DIR}/{self.hyperparameter_model_dir}")
        self.pipe = pipe

    @modal.method()
    def inference(self, text, config):
        image = self.pipe(
            text,
            num_inference_steps=config.num_inference_steps,
        ).images[0]

        # Save image to volume
        output_path = Path(MODEL_DIR) / "inference_outputs"
        output_path.mkdir(parents=True, exist_ok=True)
        filename = f"{text.replace(' ', '_')[:50]}.png"
        image.save(output_path / filename)

        return (image, text)
    

# ## Wrap the trained model in a Gradio web UI
web_app = FastAPI()


@dataclass
class AppConfig(SharedConfig):
    """Configuration information for inference."""

    num_inference_steps: int = 25
    guidance_scale: float = 6


@app.function(
    image=web_image,
    max_containers=1,
)
@modal.asgi_app()
def fastapi_app():
    import gradio as gr
    from gradio.routes import mount_gradio_app

    # Call out to the inference in a separate Modal environment with a GPU
    def go(text=""):
        if not text:
            text = example_prompts[0]
        print(text, config)
        image, prompt = Model(hyperparameter_model_dir="lr_0.0001_steps_1000_rank_16").inference.remote(
            text, config
        )
        return image

    # set up AppConfig
    config = AppConfig()


    example_prompts = [
        "A bioicon style illustration of a cat with rainbow-colored stripes",
        "A bioicon style illustration of a shark riding a skateboard",
        "A bioicon style illustration of a rabbit with glowing eyes in a futuristic cyberpunk forest",
        "A bioicon style illustration of a raccoon playing a tiny grand piano",
    ]

    # add a gradio UI around inference
    with gr.Blocks( title="Bioicons Dreambooth on Modal"
    ) as interface:
        gr.Markdown(
            "# Dream up images in the style of Bioicon icons",
        )
        with gr.Row():
            inp = gr.Textbox(  # input text component
                label="",
                placeholder="Describe what you would like to see in Bioicon style",
                lines=10,
            )
            out = gr.Image(  # output image component
                height=1024, width=1024, label="", min_width=1024, elem_id="output"
            )
        with gr.Row():
            btn = gr.Button("Dream", variant="primary", scale=2)
            btn.click(
                fn=go, inputs=inp, outputs=out
            )  # connect inputs and outputs with inference function

        with gr.Column(variant="compact"):
            # add in a few examples to inspire users
            for ii, prompt in enumerate(example_prompts):
                btn = gr.Button(prompt, variant="secondary")
                btn.click(fn=lambda idx=ii: example_prompts[idx], outputs=inp)

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=interface,
        path="/",
    )

# orchestrate (run from CLI)
@app.local_entrypoint()
def run(  # add more config params here to make training configurable
    max_train_steps: int = 250,
):
    import os
    import wandb

    sweep_config = SweepConfig()
    app_config = AppConfig()
    configs = generate_sweep_configs(sweep_config)

    results_by_rank = {}  # Dictionary to store results for each rank

    # Use Modal's starmap to run training in parallel
    with wandb.init(
        project="flux-lora-sweep-bioicons-01-26",
        name="clean_sweep_1e-4",
    ) as run:
        for config in train.map(configs):
            if config is None:
                continue  # skip invalid configs
            if config.get("learning_rate") != 1e-4:
                continue

            hyperparameter_model_dir = f"lr_{config['learning_rate']}_steps_{config['max_train_steps']}_rank_{config['rank']}"
            for image, prompt in Model(hyperparameter_model_dir=
                hyperparameter_model_dir
            ).inference.starmap(
                [(x, app_config) for x in sweep_config.bioicon_test_prompts]
            ):
                rank = config["rank"]
                steps = config["max_train_steps"]

                if rank not in results_by_rank:
                    results_by_rank[rank] = {}

                if prompt not in results_by_rank[rank]:
                    results_by_rank[rank][prompt] = {}

                results_by_rank[rank][prompt][steps] = wandb.Image(image)

        # Log a separate table for each rank
        for rank, prompt_data in results_by_rank.items():
            my_table = wandb.Table(
                columns=["Prompt"]
                + [str(steps) for steps in sweep_config.train_steps],
                data=[],
            )
            for prompt in sweep_config.bioicon_test_prompts:
                row = [prompt]
                for steps in sweep_config.train_steps:
                    row.append(
                        prompt_data.get(prompt, {}).get(steps, None)
                    )  # Get image or None if not available
                my_table.add_data(*row)
            run.log({f"results_table_rank_{rank}": my_table})
